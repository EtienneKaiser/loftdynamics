{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import functions\n",
    "reload(functions)\n",
    "from functions import get_flight_ids, load_json_file,load_parquet_file,process_json_file,  process_parquet_file, remove_constant_columns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Split Train-Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['08412468-26ac-4777-9afb-4671f426277b', '0b3f3902-2c04-4625-8576-3bb963e3d709', '0bbf0c4e-fb3c-4213-bff8-ef21ee5ebf79', '1675a16e-a2a3-4038-9007-50b0b26a685c', '21194a58-8a4d-4d0f-a4a9-e374393183b4', '28bd3cd3-1d6a-403f-ab8a-83efaf260dd0', '2a2467dd-9cb2-45de-a64f-f182395b3d1a', '39b2c145-c49f-470b-8280-d253fa98153f', '663f573a-74c5-4368-b60b-1fb433cd835d', '8ac99efe-b70b-4b4b-983b-6064fe37c67b', '8c36586f-94e9-4ae9-8384-0f3342008677', '92b2d28b-21e4-498c-b6dd-c27a47716a25', '9a2e5b24-1d93-47ef-bd90-0fae0d719df7', 'a366ff0e-ac1e-4632-821f-594ee8750b90', 'a376807a-82d3-4526-b19f-98d4b3f9078b', 'b5a540db-434b-4c3d-86dd-4668d40586c2', 'd76bb0eb-bc08-4b35-8c1f-37369452083d', 'ef4852a4-fcfe-429b-b753-d11e2ad08cac', 'f40f71de-5cc2-4719-8a5a-abcf950cbd71']\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data' \n",
    "flight_ids = get_flight_ids(data_dir)\n",
    "flight_ids.remove('StateDescriptions')\n",
    "print(flight_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = [\n",
    "    \"0b3f3902-2c04-4625-8576-3bb963e3d709\",\n",
    "    #\"663f573a-74c5-4368-b60b-1fb433cd835d\",\n",
    "    #\"8c36586f-94e9-4ae9-8384-0f3342008677\",\n",
    "    #\"a376807a-82d3-4526-b19f-98d4b3f9078b\",\n",
    "    #\"d76bb0eb-bc08-4b35-8c1f-37369452083d\",\n",
    "    #\"f40f71de-5cc2-4719-8a5a-abcf950cbd71\"\n",
    "]\n",
    "\n",
    "#put the rest into train_ids first\n",
    "train_ids = []\n",
    "for i in flight_ids:\n",
    "    if i not in test_ids:\n",
    "        train_ids.append(i)\n",
    "\n",
    "#split train_ids into train and validation ids\n",
    "\n",
    "train_ids = [\n",
    "    '08412468-26ac-4777-9afb-4671f426277b', \n",
    "    #'0bbf0c4e-fb3c-4213-bff8-ef21ee5ebf79', \n",
    "    #'1675a16e-a2a3-4038-9007-50b0b26a685c', \n",
    "    #'21194a58-8a4d-4d0f-a4a9-e374393183b4', \n",
    "    #'28bd3cd3-1d6a-403f-ab8a-83efaf260dd0', \n",
    "    #'2a2467dd-9cb2-45de-a64f-f182395b3d1a', \n",
    "    #'39b2c145-c49f-470b-8280-d253fa98153f', \n",
    "    #'8ac99efe-b70b-4b4b-983b-6064fe37c67b', \n",
    "    #'92b2d28b-21e4-498c-b6dd-c27a47716a25',\n",
    "    #'9a2e5b24-1d93-47ef-bd90-0fae0d719df7'\n",
    "    ]\n",
    "\n",
    "validate_ids=[\n",
    "    'a366ff0e-ac1e-4632-821f-594ee8750b90',\n",
    "    #'b5a540db-434b-4c3d-86dd-4668d40586c2', \n",
    "    #'ef4852a4-fcfe-429b-b753-d11e2ad08cac'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train Data Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Process and save the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a366ff0e-ac1e-4632-821f-594ee8750b90 data to validate_data\\a366ff0e-ac1e-4632-821f-594ee8750b90.csv\n",
      "Saved b5a540db-434b-4c3d-86dd-4668d40586c2 data to validate_data\\b5a540db-434b-4c3d-86dd-4668d40586c2.csv\n",
      "Saved ef4852a4-fcfe-429b-b753-d11e2ad08cac data to validate_data\\ef4852a4-fcfe-429b-b753-d11e2ad08cac.csv\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "output_dir = 'train_data'\n",
    "current_ids = train_ids\n",
    "\n",
    "#output_dir = 'validate_data'\n",
    "#current_ids = validate_ids\n",
    "\n",
    "#output_dir = 'test_data'\n",
    "#current_ids = test_ids\n",
    "\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "def save_json_data(flight_id):\n",
    "    json_path = os.path.join(data_dir, f\"{flight_id}.json\")\n",
    "\n",
    "    df = load_json_file(json_path)\n",
    "    \n",
    "    df = process_json_file(df)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"{flight_id}.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved {flight_id} data to {output_path}\")\n",
    "\n",
    "for flight_id in current_ids:\n",
    "    save_json_data(flight_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached data to ./cache/validate_data\\a366ff0e-ac1e-4632-821f-594ee8750b90_json.joblib\n",
      "Cached data to ./cache/validate_data\\b5a540db-434b-4c3d-86dd-4668d40586c2_json.joblib\n",
      "Cached data to ./cache/validate_data\\ef4852a4-fcfe-429b-b753-d11e2ad08cac_json.joblib\n"
     ]
    }
   ],
   "source": [
    "# cache the processed json file\n",
    "cache_dir = './cache/' + output_dir  \n",
    "os.makedirs(cache_dir, exist_ok=True) \n",
    "\n",
    "# cache parquet_csv file\n",
    "for flight_id in current_ids:\n",
    "    file_path = os.path.join('./' + output_dir, f\"{flight_id}.csv\")\n",
    "\n",
    "    cache_file = os.path.join(cache_dir, os.path.basename(file_path).replace('.csv', '_json.joblib'))\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"{cache_file} cached already\")\n",
    "    else:\n",
    "        data = pd.read_csv(file_path)\n",
    "        dump(data, cache_file)\n",
    "        print(f\"Cached data to {cache_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Analyze StateDescriptions\n",
    "Before we deal with the parquet files, first we have to analyze the StateDescription.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_state_descriptions(json_path):\n",
    "    # Load JSON file\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Flatten JSON data and create a DataFrame\n",
    "    records = []\n",
    "    for name, entry in data.items():\n",
    "        record = {\n",
    "            'Name': name,\n",
    "            'DataType': entry.get('dataType', 'Unknown'),\n",
    "            'Unit': entry.get('unit', 'Unknown'),\n",
    "            'StateID': entry.get('stateId', 'Unknown'),\n",
    "            'Persistence': entry.get('isPersistent', False),\n",
    "        }\n",
    "        \n",
    "        # Add default values for different models\n",
    "        for model, default_value in entry.get('DefaultValues', {}).items():\n",
    "            record[f'Default_{model}'] = default_value\n",
    "\n",
    "        # Add display units for different models\n",
    "        for model, display_unit in entry.get('DefaultDisplayUnits', {}).items():\n",
    "            record[f'Unit_{model}'] = display_unit\n",
    "\n",
    "        records.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Display general information about the DataFrame\n",
    "    print(\"General Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Display summary statistics for each column\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "    # Count unique data types and their frequency\n",
    "    print(\"\\nData Type Counts:\")\n",
    "    print(df['DataType'].value_counts())\n",
    "    \n",
    "    # Count unique units and their frequency\n",
    "    print(\"\\nUnit Counts:\")\n",
    "    print(df['Unit'].value_counts())\n",
    "    \n",
    "    # Check persistence flag distribution\n",
    "    print(\"\\nPersistence Flag Distribution:\")\n",
    "    print(df['Persistence'].value_counts())\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1746 entries, 0 to 1745\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Name         1746 non-null   object\n",
      " 1   DataType     1746 non-null   object\n",
      " 2   Unit         381 non-null    object\n",
      " 3   StateID      1746 non-null   int64 \n",
      " 4   Persistence  1746 non-null   bool  \n",
      "dtypes: bool(1), int64(1), object(3)\n",
      "memory usage: 56.4+ KB\n",
      "None\n",
      "\n",
      "Summary Statistics:\n",
      "                      Name DataType Unit       StateID Persistence\n",
      "count                 1746     1746  381  1.746000e+03        1746\n",
      "unique                1746        8   22           NaN           2\n",
      "top     VideoStream_Active   Double  0-1           NaN       False\n",
      "freq                     1     1162   99           NaN        1424\n",
      "mean                   NaN      NaN  NaN  2.118100e+09         NaN\n",
      "std                    NaN      NaN  NaN  1.232422e+09         NaN\n",
      "min                    NaN      NaN  NaN  3.513755e+06         NaN\n",
      "25%                    NaN      NaN  NaN  1.048420e+09         NaN\n",
      "50%                    NaN      NaN  NaN  2.131877e+09         NaN\n",
      "75%                    NaN      NaN  NaN  3.186254e+09         NaN\n",
      "max                    NaN      NaN  NaN  4.294954e+09         NaN\n",
      "\n",
      "Data Type Counts:\n",
      "DataType\n",
      "Double     1162\n",
      "Boolean     463\n",
      "Int32        51\n",
      "Double3      32\n",
      "String       18\n",
      "Double2      13\n",
      "Double5       4\n",
      "Double4       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unit Counts:\n",
      "Unit\n",
      "0-1           99\n",
      "m             72\n",
      "kg            46\n",
      "m/s           30\n",
      "rad           26\n",
      "no-value      21\n",
      "s             16\n",
      "Pa            15\n",
      "K             14\n",
      "visibility    12\n",
      "A              5\n",
      "Hz             4\n",
      "rad/s          4\n",
      "mm             3\n",
      "°C             3\n",
      "V              3\n",
      "%              2\n",
      "m/s²           2\n",
      "kg/s           1\n",
      "fraction       1\n",
      "°              1\n",
      "fps            1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Persistence Flag Distribution:\n",
      "Persistence\n",
      "False    1424\n",
      "True      322\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data saved to ./state_descriptions_analysis.csv\n",
      "\n",
      "First few rows of the data:\n",
      "                                    Name DataType  Unit     StateID  \\\n",
      "0                    AddOn_CockpitConfig   String  None  1514024368   \n",
      "1  Aerofly_Out_Aircraft_PressureAltitude   Double     m  2546226082   \n",
      "2            Aerofly_In_Aircraft_Options   String  None  1011097368   \n",
      "3           Aerofly_In_Aircraft_Painting   String  None   115676304   \n",
      "4         Aerofly_In_Aircraft_WindShearX   Double  None  1359970420   \n",
      "\n",
      "   Persistence  \n",
      "0        False  \n",
      "1        False  \n",
      "2        False  \n",
      "3        False  \n",
      "4        False  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "json_path = './data/StateDescriptions.json'\n",
    "df_states = analyze_state_descriptions(json_path)\n",
    "\n",
    "# save to file\n",
    "output_path = './state_descriptions_analysis.csv'\n",
    "df_states.to_csv(output_path, index=False)\n",
    "print(f\"\\nData saved to {output_path}\")\n",
    "\n",
    "# Display the first few rows for inspection\n",
    "print(\"\\nFirst few rows of the data:\")\n",
    "print(df_states.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Process and save parquet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Cache parquet raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached data to ./cache/raw\\08412468-26ac-4777-9afb-4671f426277b.joblib\n",
      "Cached data to ./cache/raw\\0b3f3902-2c04-4625-8576-3bb963e3d709.joblib\n",
      "Cached data to ./cache/raw\\0bbf0c4e-fb3c-4213-bff8-ef21ee5ebf79.joblib\n",
      "Cached data to ./cache/raw\\1675a16e-a2a3-4038-9007-50b0b26a685c.joblib\n",
      "Cached data to ./cache/raw\\21194a58-8a4d-4d0f-a4a9-e374393183b4.joblib\n",
      "Cached data to ./cache/raw\\28bd3cd3-1d6a-403f-ab8a-83efaf260dd0.joblib\n",
      "Cached data to ./cache/raw\\2a2467dd-9cb2-45de-a64f-f182395b3d1a.joblib\n",
      "Cached data to ./cache/raw\\39b2c145-c49f-470b-8280-d253fa98153f.joblib\n",
      "Cached data to ./cache/raw\\663f573a-74c5-4368-b60b-1fb433cd835d.joblib\n",
      "Cached data to ./cache/raw\\8ac99efe-b70b-4b4b-983b-6064fe37c67b.joblib\n",
      "Cached data to ./cache/raw\\8c36586f-94e9-4ae9-8384-0f3342008677.joblib\n",
      "Cached data to ./cache/raw\\92b2d28b-21e4-498c-b6dd-c27a47716a25.joblib\n",
      "Cached data to ./cache/raw\\9a2e5b24-1d93-47ef-bd90-0fae0d719df7.joblib\n",
      "Cached data to ./cache/raw\\a366ff0e-ac1e-4632-821f-594ee8750b90.joblib\n",
      "Cached data to ./cache/raw\\a376807a-82d3-4526-b19f-98d4b3f9078b.joblib\n",
      "Cached data to ./cache/raw\\b5a540db-434b-4c3d-86dd-4668d40586c2.joblib\n",
      "Cached data to ./cache/raw\\d76bb0eb-bc08-4b35-8c1f-37369452083d.joblib\n",
      "Cached data to ./cache/raw\\ef4852a4-fcfe-429b-b753-d11e2ad08cac.joblib\n",
      "Cached data to ./cache/raw\\f40f71de-5cc2-4719-8a5a-abcf950cbd71.joblib\n"
     ]
    }
   ],
   "source": [
    "cache_dir = './cache/raw'  \n",
    "os.makedirs(cache_dir, exist_ok=True) \n",
    "\n",
    "# cache parquet_csv file\n",
    "for flight_id in flight_ids:\n",
    "    file_path = os.path.join('./data', f\"{flight_id}.parquet\")\n",
    "\n",
    "    cache_file = os.path.join(cache_dir, os.path.basename(file_path).replace('.parquet', '.joblib'))\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"{cache_file} cached already\")\n",
    "    else:\n",
    "        data = pd.read_parquet(file_path)\n",
    "        dump(data, cache_file)\n",
    "        print(f\"Cached data to {cache_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 process parquet data and cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.1 See the structures to adjust the function \"process_parquet_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStamp                         FrameCounter\n",
      "2024-02-14 06:26:55.328843+00:00  0               0.0\n",
      "2024-02-14 06:26:55.338647+00:00  1               NaN\n",
      "2024-02-14 06:26:57.816582+00:00  2               NaN\n",
      "2024-02-14 06:26:57.831172+00:00  3               NaN\n",
      "2024-02-14 06:26:57.859734+00:00  4               NaN\n",
      "                                                 ... \n",
      "2024-02-14 06:45:40.456558+00:00  289727          NaN\n",
      "2024-02-14 06:45:40.469469+00:00  289728          NaN\n",
      "2024-02-14 06:45:40.488957+00:00  289729          NaN\n",
      "2024-02-14 06:45:40.502970+00:00  289730          NaN\n",
      "2024-02-14 06:45:40.510829+00:00  289731          NaN\n",
      "Name: 3501046967, Length: 205635, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# see the original structure os the parquet file\n",
    "data = pd.read_parquet('./data/2a2467dd-9cb2-45de-a64f-f182395b3d1a.parquet')\n",
    "print(data[3501046967])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicoh\\Documents\\OST\\HS24\\StatML\\3_LoftDynamics\\loftdynamics\\data_process\\functions.py:108: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill')\n",
      "c:\\Users\\nicoh\\Documents\\OST\\HS24\\StatML\\3_LoftDynamics\\loftdynamics\\data_process\\functions.py:108: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                    6417134     \\\n",
      "TimeStamp                        FrameCounter                                                    \n",
      "2024-02-14 06:46:58.397576+00:00 0             [1.0, 0.17888563049853373, 0.19257086999022482]   \n",
      "2024-02-14 06:46:58.416348+00:00 1             [1.0, 0.17888563049853373, 0.19257086999022482]   \n",
      "2024-02-14 06:47:00.886922+00:00 2             [1.0, 0.17888563049853373, 0.19257086999022482]   \n",
      "2024-02-14 06:47:00.894921+00:00 3             [1.0, 0.17888563049853373, 0.19257086999022482]   \n",
      "2024-02-14 06:47:00.926924+00:00 4             [1.0, 0.17888563049853373, 0.19257086999022482]   \n",
      "\n",
      "                                               6741304     28827303    \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0                  False         0.0   \n",
      "2024-02-14 06:46:58.416348+00:00 1                  False         0.0   \n",
      "2024-02-14 06:47:00.886922+00:00 2                  False         0.0   \n",
      "2024-02-14 06:47:00.894921+00:00 3                  False         0.0   \n",
      "2024-02-14 06:47:00.926924+00:00 4                  False         0.0   \n",
      "\n",
      "                                               32976648    43103889    \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0                  False       False   \n",
      "2024-02-14 06:46:58.416348+00:00 1                  False       False   \n",
      "2024-02-14 06:47:00.886922+00:00 2                  False       False   \n",
      "2024-02-14 06:47:00.894921+00:00 3                  False       False   \n",
      "2024-02-14 06:47:00.926924+00:00 4                  False       False   \n",
      "\n",
      "                                               43292983         45268905    \\\n",
      "TimeStamp                        FrameCounter                                \n",
      "2024-02-14 06:46:58.397576+00:00 0                   10.0  [0.0, 0.0, 0.0]   \n",
      "2024-02-14 06:46:58.416348+00:00 1                   10.0  [0.0, 0.0, 0.0]   \n",
      "2024-02-14 06:47:00.886922+00:00 2                   10.0  [0.0, 0.0, 0.0]   \n",
      "2024-02-14 06:47:00.894921+00:00 3                   10.0  [0.0, 0.0, 0.0]   \n",
      "2024-02-14 06:47:00.926924+00:00 4                   10.0  [0.0, 0.0, 0.0]   \n",
      "\n",
      "                                               46966142    55391156    \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0                  False         0.0   \n",
      "2024-02-14 06:46:58.416348+00:00 1                  False         0.0   \n",
      "2024-02-14 06:47:00.886922+00:00 2                  False         0.0   \n",
      "2024-02-14 06:47:00.894921+00:00 3                  False         0.0   \n",
      "2024-02-14 06:47:00.926924+00:00 4                  False         0.0   \n",
      "\n",
      "                                               68054868    ...  4247030081  \\\n",
      "TimeStamp                        FrameCounter              ...               \n",
      "2024-02-14 06:46:58.397576+00:00 0                   True  ...   -2.521325   \n",
      "2024-02-14 06:46:58.416348+00:00 1                   True  ...   -2.521325   \n",
      "2024-02-14 06:47:00.886922+00:00 2                   True  ...   -0.000000   \n",
      "2024-02-14 06:47:00.894921+00:00 3                   True  ...   -0.000000   \n",
      "2024-02-14 06:47:00.926924+00:00 4                   True  ...   -0.000000   \n",
      "\n",
      "                                               4250662552  4261424841  \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0                    0.0        True   \n",
      "2024-02-14 06:46:58.416348+00:00 1                    0.0        True   \n",
      "2024-02-14 06:47:00.886922+00:00 2                    0.0        True   \n",
      "2024-02-14 06:47:00.894921+00:00 3                    0.0        True   \n",
      "2024-02-14 06:47:00.926924+00:00 4                    0.0        True   \n",
      "\n",
      "                                                                                   4264003232  \\\n",
      "TimeStamp                        FrameCounter                                                   \n",
      "2024-02-14 06:46:58.397576+00:00 0             [1.0, 0.9208211143695014, 0.12316715542521994]   \n",
      "2024-02-14 06:46:58.416348+00:00 1             [1.0, 0.9208211143695014, 0.12316715542521994]   \n",
      "2024-02-14 06:47:00.886922+00:00 2             [1.0, 0.9208211143695014, 0.12316715542521994]   \n",
      "2024-02-14 06:47:00.894921+00:00 3             [1.0, 0.9208211143695014, 0.12316715542521994]   \n",
      "2024-02-14 06:47:00.926924+00:00 4             [1.0, 0.9208211143695014, 0.12316715542521994]   \n",
      "\n",
      "                                               4279155387  4281995872  \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0                   True        True   \n",
      "2024-02-14 06:46:58.416348+00:00 1                   True        True   \n",
      "2024-02-14 06:47:00.886922+00:00 2                   True        True   \n",
      "2024-02-14 06:47:00.894921+00:00 3                   True        True   \n",
      "2024-02-14 06:47:00.926924+00:00 4                   True        True   \n",
      "\n",
      "                                               4289640958  4291312414  \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0                   h125         0.0   \n",
      "2024-02-14 06:46:58.416348+00:00 1                   h125         0.0   \n",
      "2024-02-14 06:47:00.886922+00:00 2                   h125         0.0   \n",
      "2024-02-14 06:47:00.894921+00:00 3                   h125         0.0   \n",
      "2024-02-14 06:47:00.926924+00:00 4                   h125         0.0   \n",
      "\n",
      "                                               4294814812  4294953991  \n",
      "TimeStamp                        FrameCounter                          \n",
      "2024-02-14 06:46:58.397576+00:00 0                    0.0         1.0  \n",
      "2024-02-14 06:46:58.416348+00:00 1                    0.0         1.0  \n",
      "2024-02-14 06:47:00.886922+00:00 2                    0.0         1.0  \n",
      "2024-02-14 06:47:00.894921+00:00 3                    0.0         1.0  \n",
      "2024-02-14 06:47:00.926924+00:00 4                    0.0         1.0  \n",
      "\n",
      "[5 rows x 1047 columns]\n"
     ]
    }
   ],
   "source": [
    "# See the structure of the parquet file after process\n",
    "\n",
    "df = load_parquet_file('./data/08412468-26ac-4777-9afb-4671f426277b.parquet')\n",
    "df = process_parquet_file(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the structure of the parquet file after remove unnecessay columns\n",
    "\n",
    "df= remove_constant_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               117182271   156875280   \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0               2.312171       False   \n",
      "2024-02-14 06:46:58.416348+00:00 1               2.312171       False   \n",
      "2024-02-14 06:47:00.886922+00:00 2               2.309550       False   \n",
      "2024-02-14 06:47:00.894921+00:00 3               2.309550       False   \n",
      "2024-02-14 06:47:00.926924+00:00 4               2.309550       False   \n",
      "\n",
      "                                                353872548   381802901   \\\n",
      "TimeStamp                        FrameCounter                            \n",
      "2024-02-14 06:46:58.397576+00:00 0             135500000.0         1.0   \n",
      "2024-02-14 06:46:58.416348+00:00 1             135500000.0         1.0   \n",
      "2024-02-14 06:47:00.886922+00:00 2             135500000.0         1.0   \n",
      "2024-02-14 06:47:00.894921+00:00 3             135500000.0         1.0   \n",
      "2024-02-14 06:47:00.926924+00:00 4             135500000.0         1.0   \n",
      "\n",
      "                                               513685691   518160218   \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0             509.681743       False   \n",
      "2024-02-14 06:46:58.416348+00:00 1             509.681743       False   \n",
      "2024-02-14 06:47:00.886922+00:00 2             509.950305        True   \n",
      "2024-02-14 06:47:00.894921+00:00 3             509.950305        True   \n",
      "2024-02-14 06:47:00.926924+00:00 4             509.950305        True   \n",
      "\n",
      "                                                                   557098463   \\\n",
      "TimeStamp                        FrameCounter                                   \n",
      "2024-02-14 06:46:58.397576+00:00 0             [0.0, 0.0, 0.7155849933176751]   \n",
      "2024-02-14 06:46:58.416348+00:00 1             [0.0, 0.0, 0.7155849933176751]   \n",
      "2024-02-14 06:47:00.886922+00:00 2             [0.0, 0.0, 0.7155849933176751]   \n",
      "2024-02-14 06:47:00.894921+00:00 3             [0.0, 0.0, 0.7155849933176751]   \n",
      "2024-02-14 06:47:00.926924+00:00 4             [0.0, 0.0, 0.7155849933176751]   \n",
      "\n",
      "                                               614270119   637219977   \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0               0.868611  510.285568   \n",
      "2024-02-14 06:46:58.416348+00:00 1               0.868611  510.285568   \n",
      "2024-02-14 06:47:00.886922+00:00 2               0.000000  510.556384   \n",
      "2024-02-14 06:47:00.894921+00:00 3               0.000000  510.556384   \n",
      "2024-02-14 06:47:00.926924+00:00 4               0.000000  510.556384   \n",
      "\n",
      "                                               638103778   ...  \\\n",
      "TimeStamp                        FrameCounter              ...   \n",
      "2024-02-14 06:46:58.397576+00:00 0               0.000000  ...   \n",
      "2024-02-14 06:46:58.416348+00:00 1               0.000000  ...   \n",
      "2024-02-14 06:47:00.886922+00:00 2              -0.054277  ...   \n",
      "2024-02-14 06:47:00.894921+00:00 3              -0.054280  ...   \n",
      "2024-02-14 06:47:00.926924+00:00 4              -0.028358  ...   \n",
      "\n",
      "                                                                                      3966668421  \\\n",
      "TimeStamp                        FrameCounter                                                      \n",
      "2024-02-14 06:46:58.397576+00:00 0             [0.015397662947448598, 0.2067386366654791, 9.8...   \n",
      "2024-02-14 06:46:58.416348+00:00 1             [0.015397662947448598, 0.2067386366654791, 9.8...   \n",
      "2024-02-14 06:47:00.886922+00:00 2             [-0.11874119838041874, 0.1965263554522556, 9.8...   \n",
      "2024-02-14 06:47:00.894921+00:00 3             [-0.11874081249685053, 0.19652635273154218, 9....   \n",
      "2024-02-14 06:47:00.926924+00:00 4             [-0.11874081249685053, 0.19652635273154218, 9....   \n",
      "\n",
      "                                               3981973171    4021288050  \\\n",
      "TimeStamp                        FrameCounter                             \n",
      "2024-02-14 06:46:58.397576+00:00 0                  400.0  3.857714e+06   \n",
      "2024-02-14 06:46:58.416348+00:00 1                  400.0  3.857714e+06   \n",
      "2024-02-14 06:47:00.886922+00:00 2                  400.0  4.000000e+06   \n",
      "2024-02-14 06:47:00.894921+00:00 3                  400.0  4.000000e+06   \n",
      "2024-02-14 06:47:00.926924+00:00 4                  400.0  4.000000e+06   \n",
      "\n",
      "                                               4054750327  4056189073  \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0             366.964303  284.837069   \n",
      "2024-02-14 06:46:58.416348+00:00 1             366.964303  284.837069   \n",
      "2024-02-14 06:47:00.886922+00:00 2             381.718000  284.835323   \n",
      "2024-02-14 06:47:00.894921+00:00 3             381.718000  284.835323   \n",
      "2024-02-14 06:47:00.926924+00:00 4             399.580000  284.835323   \n",
      "\n",
      "                                                                                      4058842283  \\\n",
      "TimeStamp                        FrameCounter                                                      \n",
      "2024-02-14 06:46:58.397576+00:00 0             [0.9930367276785365, 0.022116936416472353, 0.0...   \n",
      "2024-02-14 06:46:58.416348+00:00 1             [0.9929756894317355, 0.021576050603372268, 0.0...   \n",
      "2024-02-14 06:47:00.886922+00:00 2             [0.992887299721831, 0.0212184179844126, 0.0884...   \n",
      "2024-02-14 06:47:00.894921+00:00 3             [0.992887299721831, 0.0212184179844126, 0.0884...   \n",
      "2024-02-14 06:47:00.926924+00:00 4             [0.992887299721831, 0.0212184179844126, 0.0884...   \n",
      "\n",
      "                                               4066202059   4174711423  \\\n",
      "TimeStamp                        FrameCounter                            \n",
      "2024-02-14 06:46:58.397576+00:00 0               0.048876  125500000.0   \n",
      "2024-02-14 06:46:58.416348+00:00 1               0.048876  125500000.0   \n",
      "2024-02-14 06:47:00.886922+00:00 2               0.947214  125500000.0   \n",
      "2024-02-14 06:47:00.894921+00:00 3               0.947214  125500000.0   \n",
      "2024-02-14 06:47:00.926924+00:00 4               0.948192  125500000.0   \n",
      "\n",
      "                                               4247030081  \\\n",
      "TimeStamp                        FrameCounter               \n",
      "2024-02-14 06:46:58.397576+00:00 0              -2.521325   \n",
      "2024-02-14 06:46:58.416348+00:00 1              -2.521325   \n",
      "2024-02-14 06:47:00.886922+00:00 2              -0.000000   \n",
      "2024-02-14 06:47:00.894921+00:00 3              -0.000000   \n",
      "2024-02-14 06:47:00.926924+00:00 4              -0.000000   \n",
      "\n",
      "                                                                                   4264003232  \n",
      "TimeStamp                        FrameCounter                                                  \n",
      "2024-02-14 06:46:58.397576+00:00 0             [1.0, 0.9208211143695014, 0.12316715542521994]  \n",
      "2024-02-14 06:46:58.416348+00:00 1             [1.0, 0.9208211143695014, 0.12316715542521994]  \n",
      "2024-02-14 06:47:00.886922+00:00 2             [1.0, 0.9208211143695014, 0.12316715542521994]  \n",
      "2024-02-14 06:47:00.894921+00:00 3             [1.0, 0.9208211143695014, 0.12316715542521994]  \n",
      "2024-02-14 06:47:00.926924+00:00 4             [1.0, 0.9208211143695014, 0.12316715542521994]  \n",
      "\n",
      "[5 rows x 97 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.2 (Optional) Here there is some mistake that the files are saved to .csv, but remaining the same with .parquet is better. \n",
    "Since reprocessing it took a lot of time, so I just wrote the next part to transfer from csv to parquet file.\n",
    "If you haven't processed the data, skip this part and next (Convert \"_parquet.csv\" to \".parquet\"), just run the part of \" Load from cache and process parquet data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "output_dir = 'test_data'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Function to process and save Parquet file data\n",
    "def save_parquet_data(flight_id):\n",
    "    parquet_path = os.path.join(data_dir, f\"{flight_id}.parquet\")\n",
    "\n",
    "    df = load_parquet_file(parquet_path)\n",
    "     \n",
    "    df = process_parquet_file(df)  \n",
    "\n",
    "    df = remove_constant_columns(df)\n",
    "    \n",
    "    # Define the base output file name\n",
    "    output_path = os.path.join(output_dir, f\"{flight_id}_parquet.csv\")\n",
    "    \n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved {flight_id} Parquet data to {output_path}\")\n",
    "\n",
    "# Process each Parquet file and save it\n",
    "for flight_id in train_ids: \n",
    "    # check if the file exists\n",
    "    output_path = os.path.join(output_dir, f\"{flight_id}_parquet.csv\")\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"File {output_path} already exists. Skipping save.\")\n",
    "    else:\n",
    "        save_parquet_data(flight_id)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.3 (Optional) Convert \"_parquet.csv\" to \".parquet\"\n",
    "If you need to process the parquet data, skip this part. Run next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dir = \"./train_data\" \n",
    "output_dir = \"./train_data\" \n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\"_parquet.csv\"): \n",
    "        input_path = os.path.join(input_dir, file_name)\n",
    "        output_path = os.path.join(output_dir, file_name.replace(\"_parquet.csv\", \".parquet\"))\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(input_path)\n",
    "            df.to_parquet(output_path, index=False)\n",
    "            print(f\"Converted {file_name} to {output_path}\")\n",
    "            \n",
    "            os.remove(input_path)\n",
    "            print(f\"Deleted original file: {input_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.4 Load from cache and process parquet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicoh\\Documents\\OST\\HS24\\StatML\\3_LoftDynamics\\loftdynamics\\data_process\\functions.py:108: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  #df = df.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a366ff0e-ac1e-4632-821f-594ee8750b90 Parquet data to validate_data\\a366ff0e-ac1e-4632-821f-594ee8750b90.parquet\n"
     ]
    }
   ],
   "source": [
    "data_dir = './cache/raw'\n",
    "\n",
    "#output_dir = 'test_data'\n",
    "#current_ids = test_ids\n",
    "\n",
    "#output_dir = 'train_data'\n",
    "#current_ids = train_ids\n",
    "\n",
    "output_dir = 'validate_data'\n",
    "current_ids = validate_ids\n",
    "\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Function to process and save Parquet file data\n",
    "def save_parquet_data(flight_id):\n",
    "    parquet_path = os.path.join(data_dir, f\"{flight_id}.joblib\")\n",
    "\n",
    "    df = load(parquet_path)\n",
    "     \n",
    "    df = process_parquet_file(df)  \n",
    "\n",
    "    df = remove_constant_columns(df)\n",
    "    \n",
    "    # Define the base output file name\n",
    "    output_path = os.path.join(output_dir, f\"{flight_id}.parquet\")\n",
    "    \n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_parquet(output_path, index=True)\n",
    "    print(f\"Saved {flight_id} Parquet data to {output_path}\")\n",
    "\n",
    "# Process each Parquet file and save it\n",
    "for flight_id in current_ids:\n",
    "    # check if the file exists\n",
    "    output_path = os.path.join(output_dir, f\"{flight_id}.parquet\")\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"File {output_path} already exists. Skipping save.\")\n",
    "    else:\n",
    "        save_parquet_data(flight_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Cache parquet processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached data to ./cache/validate_data\\a366ff0e-ac1e-4632-821f-594ee8750b90_parquet.joblib\n"
     ]
    }
   ],
   "source": [
    "#output_dir = 'test_data'\n",
    "#current_ids = test_ids\n",
    "\n",
    "#output_dir = 'train_data'\n",
    "#current_ids = train_ids\n",
    "\n",
    "output_dir = 'validate_data'\n",
    "current_ids = validate_ids\n",
    "\n",
    "cache_dir = './cache/' + output_dir\n",
    "os.makedirs(cache_dir, exist_ok=True) \n",
    "\n",
    "# cache parquet_csv file\n",
    "for flight_id in current_ids:\n",
    "    file_path = os.path.join('./' + output_dir, f\"{flight_id}.parquet\")\n",
    "\n",
    "    cache_file = os.path.join(cache_dir, os.path.basename(file_path).replace('.parquet', '_parquet.joblib'))\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"{cache_file} cached already\")\n",
    "    else:\n",
    "        data = pd.read_parquet(file_path)\n",
    "        dump(data, cache_file)\n",
    "        print(f\"Cached data to {cache_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualize the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
