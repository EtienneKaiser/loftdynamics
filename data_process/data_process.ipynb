{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import functions\n",
    "reload(functions)\n",
    "from functions import get_flight_ids, load_json_file,load_parquet_file,process_json_file, process_parquet_file, \\\n",
    "    remove_constant_columns, columns_to_scalar, get_common_columns, add_row_label, align_columns\n",
    "import os\n",
    "import pandas as pd\n",
    "from joblib import dump, load\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Split Train-Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['08412468-26ac-4777-9afb-4671f426277b', '0b3f3902-2c04-4625-8576-3bb963e3d709', '0bbf0c4e-fb3c-4213-bff8-ef21ee5ebf79', '1675a16e-a2a3-4038-9007-50b0b26a685c', '21194a58-8a4d-4d0f-a4a9-e374393183b4', '28bd3cd3-1d6a-403f-ab8a-83efaf260dd0', '2a2467dd-9cb2-45de-a64f-f182395b3d1a', '39b2c145-c49f-470b-8280-d253fa98153f', '663f573a-74c5-4368-b60b-1fb433cd835d', '8ac99efe-b70b-4b4b-983b-6064fe37c67b', '8c36586f-94e9-4ae9-8384-0f3342008677', '92b2d28b-21e4-498c-b6dd-c27a47716a25', '9a2e5b24-1d93-47ef-bd90-0fae0d719df7', 'a366ff0e-ac1e-4632-821f-594ee8750b90', 'a376807a-82d3-4526-b19f-98d4b3f9078b', 'b5a540db-434b-4c3d-86dd-4668d40586c2', 'd76bb0eb-bc08-4b35-8c1f-37369452083d', 'ef4852a4-fcfe-429b-b753-d11e2ad08cac', 'f40f71de-5cc2-4719-8a5a-abcf950cbd71']\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data' \n",
    "flight_ids = get_flight_ids(data_dir)\n",
    "flight_ids.remove('StateDescriptions')\n",
    "print(flight_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = [\n",
    "    \"0b3f3902-2c04-4625-8576-3bb963e3d709\",\n",
    "    \"663f573a-74c5-4368-b60b-1fb433cd835d\",\n",
    "    \"8c36586f-94e9-4ae9-8384-0f3342008677\",\n",
    "    \"a376807a-82d3-4526-b19f-98d4b3f9078b\",\n",
    "    \"d76bb0eb-bc08-4b35-8c1f-37369452083d\",\n",
    "    \"f40f71de-5cc2-4719-8a5a-abcf950cbd71\"\n",
    "]\n",
    "\n",
    "#put the rest into train_ids first\n",
    "train_ids = []\n",
    "for i in flight_ids:\n",
    "    if i not in test_ids:\n",
    "        train_ids.append(i)\n",
    "\n",
    "#split train_ids into train and validation ids\n",
    "\n",
    "train_ids = [\n",
    "    '08412468-26ac-4777-9afb-4671f426277b', \n",
    "    '0bbf0c4e-fb3c-4213-bff8-ef21ee5ebf79', \n",
    "    '1675a16e-a2a3-4038-9007-50b0b26a685c', \n",
    "    '21194a58-8a4d-4d0f-a4a9-e374393183b4', \n",
    "    '28bd3cd3-1d6a-403f-ab8a-83efaf260dd0', \n",
    "    '2a2467dd-9cb2-45de-a64f-f182395b3d1a', \n",
    "    '39b2c145-c49f-470b-8280-d253fa98153f', \n",
    "    '8ac99efe-b70b-4b4b-983b-6064fe37c67b', \n",
    "    '92b2d28b-21e4-498c-b6dd-c27a47716a25',\n",
    "    '9a2e5b24-1d93-47ef-bd90-0fae0d719df7'\n",
    "    ]\n",
    "\n",
    "validate_ids=[\n",
    "    'a366ff0e-ac1e-4632-821f-594ee8750b90',\n",
    "    'b5a540db-434b-4c3d-86dd-4668d40586c2', \n",
    "    'ef4852a4-fcfe-429b-b753-d11e2ad08cac'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train Data Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Process and save the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved a366ff0e-ac1e-4632-821f-594ee8750b90 data to validate_data\\a366ff0e-ac1e-4632-821f-594ee8750b90.csv\n",
      "Saved b5a540db-434b-4c3d-86dd-4668d40586c2 data to validate_data\\b5a540db-434b-4c3d-86dd-4668d40586c2.csv\n",
      "Saved ef4852a4-fcfe-429b-b753-d11e2ad08cac data to validate_data\\ef4852a4-fcfe-429b-b753-d11e2ad08cac.csv\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "output_dir = 'train_data'\n",
    "current_ids = train_ids\n",
    "\n",
    "#output_dir = 'validate_data'\n",
    "#current_ids = validate_ids\n",
    "\n",
    "#output_dir = 'test_data'\n",
    "#current_ids = test_ids\n",
    "\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "def save_json_data(flight_id):\n",
    "    json_path = os.path.join(data_dir, f\"{flight_id}.json\")\n",
    "\n",
    "    df = load_json_file(json_path)\n",
    "    \n",
    "    df = process_json_file(df)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"{flight_id}.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved {flight_id} data to {output_path}\")\n",
    "\n",
    "for flight_id in current_ids:\n",
    "    save_json_data(flight_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached data to ./cache/validate_data\\a366ff0e-ac1e-4632-821f-594ee8750b90_json.joblib\n",
      "Cached data to ./cache/validate_data\\b5a540db-434b-4c3d-86dd-4668d40586c2_json.joblib\n",
      "Cached data to ./cache/validate_data\\ef4852a4-fcfe-429b-b753-d11e2ad08cac_json.joblib\n"
     ]
    }
   ],
   "source": [
    "# cache the processed json file\n",
    "cache_dir = './cache/' + output_dir  \n",
    "os.makedirs(cache_dir, exist_ok=True) \n",
    "\n",
    "# cache parquet_csv file\n",
    "for flight_id in current_ids:\n",
    "    file_path = os.path.join('./' + output_dir, f\"{flight_id}.csv\")\n",
    "\n",
    "    cache_file = os.path.join(cache_dir, os.path.basename(file_path).replace('.csv', '_json.joblib'))\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"{cache_file} cached already\")\n",
    "    else:\n",
    "        data = pd.read_csv(file_path)\n",
    "        dump(data, cache_file)\n",
    "        print(f\"Cached data to {cache_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Analyze StateDescriptions\n",
    "Before we deal with the parquet files, first we have to analyze the StateDescription.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_state_descriptions(json_path):\n",
    "    # Load JSON file\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Flatten JSON data and create a DataFrame\n",
    "    records = []\n",
    "    for name, entry in data.items():\n",
    "        record = {\n",
    "            'Name': name,\n",
    "            'DataType': entry.get('dataType', 'Unknown'),\n",
    "            'Unit': entry.get('unit', 'Unknown'),\n",
    "            'StateID': entry.get('stateId', 'Unknown'),\n",
    "            'Persistence': entry.get('isPersistent', False),\n",
    "        }\n",
    "        \n",
    "        # Add default values for different models\n",
    "        for model, default_value in entry.get('DefaultValues', {}).items():\n",
    "            record[f'Default_{model}'] = default_value\n",
    "\n",
    "        # Add display units for different models\n",
    "        for model, display_unit in entry.get('DefaultDisplayUnits', {}).items():\n",
    "            record[f'Unit_{model}'] = display_unit\n",
    "\n",
    "        records.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Display general information about the DataFrame\n",
    "    print(\"General Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    # Display summary statistics for each column\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "    # Count unique data types and their frequency\n",
    "    print(\"\\nData Type Counts:\")\n",
    "    print(df['DataType'].value_counts())\n",
    "    \n",
    "    # Count unique units and their frequency\n",
    "    print(\"\\nUnit Counts:\")\n",
    "    print(df['Unit'].value_counts())\n",
    "    \n",
    "    # Check persistence flag distribution\n",
    "    print(\"\\nPersistence Flag Distribution:\")\n",
    "    print(df['Persistence'].value_counts())\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1746 entries, 0 to 1745\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Name         1746 non-null   object\n",
      " 1   DataType     1746 non-null   object\n",
      " 2   Unit         381 non-null    object\n",
      " 3   StateID      1746 non-null   int64 \n",
      " 4   Persistence  1746 non-null   bool  \n",
      "dtypes: bool(1), int64(1), object(3)\n",
      "memory usage: 56.4+ KB\n",
      "None\n",
      "\n",
      "Summary Statistics:\n",
      "                      Name DataType Unit       StateID Persistence\n",
      "count                 1746     1746  381  1.746000e+03        1746\n",
      "unique                1746        8   22           NaN           2\n",
      "top     VideoStream_Active   Double  0-1           NaN       False\n",
      "freq                     1     1162   99           NaN        1424\n",
      "mean                   NaN      NaN  NaN  2.118100e+09         NaN\n",
      "std                    NaN      NaN  NaN  1.232422e+09         NaN\n",
      "min                    NaN      NaN  NaN  3.513755e+06         NaN\n",
      "25%                    NaN      NaN  NaN  1.048420e+09         NaN\n",
      "50%                    NaN      NaN  NaN  2.131877e+09         NaN\n",
      "75%                    NaN      NaN  NaN  3.186254e+09         NaN\n",
      "max                    NaN      NaN  NaN  4.294954e+09         NaN\n",
      "\n",
      "Data Type Counts:\n",
      "DataType\n",
      "Double     1162\n",
      "Boolean     463\n",
      "Int32        51\n",
      "Double3      32\n",
      "String       18\n",
      "Double2      13\n",
      "Double5       4\n",
      "Double4       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unit Counts:\n",
      "Unit\n",
      "0-1           99\n",
      "m             72\n",
      "kg            46\n",
      "m/s           30\n",
      "rad           26\n",
      "no-value      21\n",
      "s             16\n",
      "Pa            15\n",
      "K             14\n",
      "visibility    12\n",
      "A              5\n",
      "Hz             4\n",
      "rad/s          4\n",
      "mm             3\n",
      "°C             3\n",
      "V              3\n",
      "%              2\n",
      "m/s²           2\n",
      "kg/s           1\n",
      "fraction       1\n",
      "°              1\n",
      "fps            1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Persistence Flag Distribution:\n",
      "Persistence\n",
      "False    1424\n",
      "True      322\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data saved to ./state_descriptions_analysis.csv\n",
      "\n",
      "First few rows of the data:\n",
      "                                    Name DataType  Unit     StateID  \\\n",
      "0                    AddOn_CockpitConfig   String  None  1514024368   \n",
      "1  Aerofly_Out_Aircraft_PressureAltitude   Double     m  2546226082   \n",
      "2            Aerofly_In_Aircraft_Options   String  None  1011097368   \n",
      "3           Aerofly_In_Aircraft_Painting   String  None   115676304   \n",
      "4         Aerofly_In_Aircraft_WindShearX   Double  None  1359970420   \n",
      "\n",
      "   Persistence  \n",
      "0        False  \n",
      "1        False  \n",
      "2        False  \n",
      "3        False  \n",
      "4        False  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "json_path = './data/StateDescriptions.json'\n",
    "df_states = analyze_state_descriptions(json_path)\n",
    "\n",
    "# save to file\n",
    "output_path = './state_descriptions_analysis.csv'\n",
    "df_states.to_csv(output_path, index=False)\n",
    "print(f\"\\nData saved to {output_path}\")\n",
    "\n",
    "# Display the first few rows for inspection\n",
    "print(\"\\nFirst few rows of the data:\")\n",
    "print(df_states.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Process and save parquet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Cache parquet raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached data to ./cache/raw\\08412468-26ac-4777-9afb-4671f426277b.joblib\n",
      "Cached data to ./cache/raw\\0b3f3902-2c04-4625-8576-3bb963e3d709.joblib\n",
      "Cached data to ./cache/raw\\0bbf0c4e-fb3c-4213-bff8-ef21ee5ebf79.joblib\n",
      "Cached data to ./cache/raw\\1675a16e-a2a3-4038-9007-50b0b26a685c.joblib\n",
      "Cached data to ./cache/raw\\21194a58-8a4d-4d0f-a4a9-e374393183b4.joblib\n",
      "Cached data to ./cache/raw\\28bd3cd3-1d6a-403f-ab8a-83efaf260dd0.joblib\n",
      "Cached data to ./cache/raw\\2a2467dd-9cb2-45de-a64f-f182395b3d1a.joblib\n",
      "Cached data to ./cache/raw\\39b2c145-c49f-470b-8280-d253fa98153f.joblib\n",
      "Cached data to ./cache/raw\\663f573a-74c5-4368-b60b-1fb433cd835d.joblib\n",
      "Cached data to ./cache/raw\\8ac99efe-b70b-4b4b-983b-6064fe37c67b.joblib\n",
      "Cached data to ./cache/raw\\8c36586f-94e9-4ae9-8384-0f3342008677.joblib\n",
      "Cached data to ./cache/raw\\92b2d28b-21e4-498c-b6dd-c27a47716a25.joblib\n",
      "Cached data to ./cache/raw\\9a2e5b24-1d93-47ef-bd90-0fae0d719df7.joblib\n",
      "Cached data to ./cache/raw\\a366ff0e-ac1e-4632-821f-594ee8750b90.joblib\n",
      "Cached data to ./cache/raw\\a376807a-82d3-4526-b19f-98d4b3f9078b.joblib\n",
      "Cached data to ./cache/raw\\b5a540db-434b-4c3d-86dd-4668d40586c2.joblib\n",
      "Cached data to ./cache/raw\\d76bb0eb-bc08-4b35-8c1f-37369452083d.joblib\n",
      "Cached data to ./cache/raw\\ef4852a4-fcfe-429b-b753-d11e2ad08cac.joblib\n",
      "Cached data to ./cache/raw\\f40f71de-5cc2-4719-8a5a-abcf950cbd71.joblib\n"
     ]
    }
   ],
   "source": [
    "cache_dir = './cache/raw'  \n",
    "os.makedirs(cache_dir, exist_ok=True) \n",
    "\n",
    "# cache parquet_csv file\n",
    "for flight_id in flight_ids:\n",
    "    file_path = os.path.join('./data', f\"{flight_id}.parquet\")\n",
    "\n",
    "    cache_file = os.path.join(cache_dir, os.path.basename(file_path).replace('.parquet', '.joblib'))\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"{cache_file} cached already\")\n",
    "    else:\n",
    "        data = pd.read_parquet(file_path)\n",
    "        dump(data, cache_file)\n",
    "        print(f\"Cached data to {cache_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 process parquet data and cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.1 See the structures to adjust the function \"process_parquet_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStamp                         FrameCounter\n",
      "2024-02-14 06:26:55.328843+00:00  0               0.0\n",
      "2024-02-14 06:26:55.338647+00:00  1               NaN\n",
      "2024-02-14 06:26:57.816582+00:00  2               NaN\n",
      "2024-02-14 06:26:57.831172+00:00  3               NaN\n",
      "2024-02-14 06:26:57.859734+00:00  4               NaN\n",
      "                                                 ... \n",
      "2024-02-14 06:45:40.456558+00:00  289727          NaN\n",
      "2024-02-14 06:45:40.469469+00:00  289728          NaN\n",
      "2024-02-14 06:45:40.488957+00:00  289729          NaN\n",
      "2024-02-14 06:45:40.502970+00:00  289730          NaN\n",
      "2024-02-14 06:45:40.510829+00:00  289731          NaN\n",
      "Name: 3501046967, Length: 205635, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# see the original structure os the parquet file\n",
    "data = pd.read_parquet('./data/2a2467dd-9cb2-45de-a64f-f182395b3d1a.parquet')\n",
    "print(data[3501046967])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the structure of the parquet file after process\n",
    "\n",
    "df = load_parquet_file('./data/08412468-26ac-4777-9afb-4671f426277b.parquet')\n",
    "df = process_parquet_file(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the structure of the parquet file after remove unnecessay columns\n",
    "\n",
    "df= remove_constant_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               117182271   156875280   \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0               2.312171       False   \n",
      "2024-02-14 06:46:58.416348+00:00 1               2.312171       False   \n",
      "2024-02-14 06:47:00.886922+00:00 2               2.309550       False   \n",
      "2024-02-14 06:47:00.894921+00:00 3               2.309550       False   \n",
      "2024-02-14 06:47:00.926924+00:00 4               2.309550       False   \n",
      "\n",
      "                                                353872548   381802901   \\\n",
      "TimeStamp                        FrameCounter                            \n",
      "2024-02-14 06:46:58.397576+00:00 0             135500000.0         1.0   \n",
      "2024-02-14 06:46:58.416348+00:00 1             135500000.0         1.0   \n",
      "2024-02-14 06:47:00.886922+00:00 2             135500000.0         1.0   \n",
      "2024-02-14 06:47:00.894921+00:00 3             135500000.0         1.0   \n",
      "2024-02-14 06:47:00.926924+00:00 4             135500000.0         1.0   \n",
      "\n",
      "                                               513685691   518160218   \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0             509.681743       False   \n",
      "2024-02-14 06:46:58.416348+00:00 1             509.681743       False   \n",
      "2024-02-14 06:47:00.886922+00:00 2             509.950305        True   \n",
      "2024-02-14 06:47:00.894921+00:00 3             509.950305        True   \n",
      "2024-02-14 06:47:00.926924+00:00 4             509.950305        True   \n",
      "\n",
      "                                                                   557098463   \\\n",
      "TimeStamp                        FrameCounter                                   \n",
      "2024-02-14 06:46:58.397576+00:00 0             [0.0, 0.0, 0.7155849933176751]   \n",
      "2024-02-14 06:46:58.416348+00:00 1             [0.0, 0.0, 0.7155849933176751]   \n",
      "2024-02-14 06:47:00.886922+00:00 2             [0.0, 0.0, 0.7155849933176751]   \n",
      "2024-02-14 06:47:00.894921+00:00 3             [0.0, 0.0, 0.7155849933176751]   \n",
      "2024-02-14 06:47:00.926924+00:00 4             [0.0, 0.0, 0.7155849933176751]   \n",
      "\n",
      "                                               614270119   637219977   \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0               0.868611  510.285568   \n",
      "2024-02-14 06:46:58.416348+00:00 1               0.868611  510.285568   \n",
      "2024-02-14 06:47:00.886922+00:00 2               0.000000  510.556384   \n",
      "2024-02-14 06:47:00.894921+00:00 3               0.000000  510.556384   \n",
      "2024-02-14 06:47:00.926924+00:00 4               0.000000  510.556384   \n",
      "\n",
      "                                               638103778   ...  \\\n",
      "TimeStamp                        FrameCounter              ...   \n",
      "2024-02-14 06:46:58.397576+00:00 0               0.000000  ...   \n",
      "2024-02-14 06:46:58.416348+00:00 1               0.000000  ...   \n",
      "2024-02-14 06:47:00.886922+00:00 2              -0.054277  ...   \n",
      "2024-02-14 06:47:00.894921+00:00 3              -0.054280  ...   \n",
      "2024-02-14 06:47:00.926924+00:00 4              -0.028358  ...   \n",
      "\n",
      "                                                                                      3966668421  \\\n",
      "TimeStamp                        FrameCounter                                                      \n",
      "2024-02-14 06:46:58.397576+00:00 0             [0.015397662947448598, 0.2067386366654791, 9.8...   \n",
      "2024-02-14 06:46:58.416348+00:00 1             [0.015397662947448598, 0.2067386366654791, 9.8...   \n",
      "2024-02-14 06:47:00.886922+00:00 2             [-0.11874119838041874, 0.1965263554522556, 9.8...   \n",
      "2024-02-14 06:47:00.894921+00:00 3             [-0.11874081249685053, 0.19652635273154218, 9....   \n",
      "2024-02-14 06:47:00.926924+00:00 4             [-0.11874081249685053, 0.19652635273154218, 9....   \n",
      "\n",
      "                                               3981973171    4021288050  \\\n",
      "TimeStamp                        FrameCounter                             \n",
      "2024-02-14 06:46:58.397576+00:00 0                  400.0  3.857714e+06   \n",
      "2024-02-14 06:46:58.416348+00:00 1                  400.0  3.857714e+06   \n",
      "2024-02-14 06:47:00.886922+00:00 2                  400.0  4.000000e+06   \n",
      "2024-02-14 06:47:00.894921+00:00 3                  400.0  4.000000e+06   \n",
      "2024-02-14 06:47:00.926924+00:00 4                  400.0  4.000000e+06   \n",
      "\n",
      "                                               4054750327  4056189073  \\\n",
      "TimeStamp                        FrameCounter                           \n",
      "2024-02-14 06:46:58.397576+00:00 0             366.964303  284.837069   \n",
      "2024-02-14 06:46:58.416348+00:00 1             366.964303  284.837069   \n",
      "2024-02-14 06:47:00.886922+00:00 2             381.718000  284.835323   \n",
      "2024-02-14 06:47:00.894921+00:00 3             381.718000  284.835323   \n",
      "2024-02-14 06:47:00.926924+00:00 4             399.580000  284.835323   \n",
      "\n",
      "                                                                                      4058842283  \\\n",
      "TimeStamp                        FrameCounter                                                      \n",
      "2024-02-14 06:46:58.397576+00:00 0             [0.9930367276785365, 0.022116936416472353, 0.0...   \n",
      "2024-02-14 06:46:58.416348+00:00 1             [0.9929756894317355, 0.021576050603372268, 0.0...   \n",
      "2024-02-14 06:47:00.886922+00:00 2             [0.992887299721831, 0.0212184179844126, 0.0884...   \n",
      "2024-02-14 06:47:00.894921+00:00 3             [0.992887299721831, 0.0212184179844126, 0.0884...   \n",
      "2024-02-14 06:47:00.926924+00:00 4             [0.992887299721831, 0.0212184179844126, 0.0884...   \n",
      "\n",
      "                                               4066202059   4174711423  \\\n",
      "TimeStamp                        FrameCounter                            \n",
      "2024-02-14 06:46:58.397576+00:00 0               0.048876  125500000.0   \n",
      "2024-02-14 06:46:58.416348+00:00 1               0.048876  125500000.0   \n",
      "2024-02-14 06:47:00.886922+00:00 2               0.947214  125500000.0   \n",
      "2024-02-14 06:47:00.894921+00:00 3               0.947214  125500000.0   \n",
      "2024-02-14 06:47:00.926924+00:00 4               0.948192  125500000.0   \n",
      "\n",
      "                                               4247030081  \\\n",
      "TimeStamp                        FrameCounter               \n",
      "2024-02-14 06:46:58.397576+00:00 0              -2.521325   \n",
      "2024-02-14 06:46:58.416348+00:00 1              -2.521325   \n",
      "2024-02-14 06:47:00.886922+00:00 2              -0.000000   \n",
      "2024-02-14 06:47:00.894921+00:00 3              -0.000000   \n",
      "2024-02-14 06:47:00.926924+00:00 4              -0.000000   \n",
      "\n",
      "                                                                                   4264003232  \n",
      "TimeStamp                        FrameCounter                                                  \n",
      "2024-02-14 06:46:58.397576+00:00 0             [1.0, 0.9208211143695014, 0.12316715542521994]  \n",
      "2024-02-14 06:46:58.416348+00:00 1             [1.0, 0.9208211143695014, 0.12316715542521994]  \n",
      "2024-02-14 06:47:00.886922+00:00 2             [1.0, 0.9208211143695014, 0.12316715542521994]  \n",
      "2024-02-14 06:47:00.894921+00:00 3             [1.0, 0.9208211143695014, 0.12316715542521994]  \n",
      "2024-02-14 06:47:00.926924+00:00 4             [1.0, 0.9208211143695014, 0.12316715542521994]  \n",
      "\n",
      "[5 rows x 97 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.2 (Optional) Here there is some mistake that the files are saved to .csv, but remaining the same with .parquet is better. \n",
    "Since reprocessing it took a lot of time, so I just wrote the next part to transfer from csv to parquet file.\n",
    "If you haven't processed the data, skip this part and next (Convert \"_parquet.csv\" to \".parquet\"), just run the part of \" Load from cache and process parquet data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "output_dir = 'test_data'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Function to process and save Parquet file data\n",
    "def save_parquet_data(flight_id):\n",
    "    parquet_path = os.path.join(data_dir, f\"{flight_id}.parquet\")\n",
    "\n",
    "    df = load_parquet_file(parquet_path)\n",
    "     \n",
    "    df = process_parquet_file(df)\n",
    "\n",
    "    df = columns_to_scalar(df) # [] -> float | string -> NaN | bool -> int | ...\n",
    "\n",
    "    df = remove_constant_columns(df)\n",
    "    \n",
    "    # Define the base output file name\n",
    "    output_path = os.path.join(output_dir, f\"{flight_id}_parquet.csv\")\n",
    "    \n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved {flight_id} Parquet data to {output_path}\")\n",
    "\n",
    "# Process each Parquet file and save it\n",
    "for flight_id in train_ids: \n",
    "    # check if the file exists\n",
    "    output_path = os.path.join(output_dir, f\"{flight_id}_parquet.csv\")\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"File {output_path} already exists. Skipping save.\")\n",
    "    else:\n",
    "        save_parquet_data(flight_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.3 (Optional) Convert \"_parquet.csv\" to \".parquet\"\n",
    "If you need to process the parquet data, skip this part. Run next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dir = \"./train_data\" \n",
    "output_dir = \"./train_data\" \n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith(\"_parquet.csv\"): \n",
    "        input_path = os.path.join(input_dir, file_name)\n",
    "        output_path = os.path.join(output_dir, file_name.replace(\"_parquet.csv\", \".parquet\"))\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(input_path)\n",
    "            df.to_parquet(output_path, index=False)\n",
    "            print(f\"Converted {file_name} to {output_path}\")\n",
    "            \n",
    "            os.remove(input_path)\n",
    "            print(f\"Deleted original file: {input_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.4 Load from cache and process parquet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File test_data\\0b3f3902-2c04-4625-8576-3bb963e3d709.parquet already exists. Skipping save.\n",
      "2024-11-24 04:43:04.796253: Processing flight 663f573a-74c5-4368-b60b-1fb433cd835d\n",
      "2024-11-24 04:47:34.462837: Saved 663f573a-74c5-4368-b60b-1fb433cd835d Parquet data to test_data\\663f573a-74c5-4368-b60b-1fb433cd835d.parquet\n",
      "2024-11-24 04:47:34.535458: Processing flight 8c36586f-94e9-4ae9-8384-0f3342008677\n",
      "2024-11-24 04:53:08.580219: Saved 8c36586f-94e9-4ae9-8384-0f3342008677 Parquet data to test_data\\8c36586f-94e9-4ae9-8384-0f3342008677.parquet\n",
      "2024-11-24 04:53:08.624396: Processing flight a376807a-82d3-4526-b19f-98d4b3f9078b\n",
      "2024-11-24 04:57:56.993207: Saved a376807a-82d3-4526-b19f-98d4b3f9078b Parquet data to test_data\\a376807a-82d3-4526-b19f-98d4b3f9078b.parquet\n",
      "File test_data\\d76bb0eb-bc08-4b35-8c1f-37369452083d.parquet already exists. Skipping save.\n",
      "File test_data\\f40f71de-5cc2-4719-8a5a-abcf950cbd71.parquet already exists. Skipping save.\n",
      "File train_data\\08412468-26ac-4777-9afb-4671f426277b.parquet already exists. Skipping save.\n",
      "File train_data\\0bbf0c4e-fb3c-4213-bff8-ef21ee5ebf79.parquet already exists. Skipping save.\n",
      "File train_data\\1675a16e-a2a3-4038-9007-50b0b26a685c.parquet already exists. Skipping save.\n",
      "File train_data\\21194a58-8a4d-4d0f-a4a9-e374393183b4.parquet already exists. Skipping save.\n",
      "File train_data\\28bd3cd3-1d6a-403f-ab8a-83efaf260dd0.parquet already exists. Skipping save.\n",
      "File train_data\\2a2467dd-9cb2-45de-a64f-f182395b3d1a.parquet already exists. Skipping save.\n",
      "File train_data\\39b2c145-c49f-470b-8280-d253fa98153f.parquet already exists. Skipping save.\n",
      "File train_data\\8ac99efe-b70b-4b4b-983b-6064fe37c67b.parquet already exists. Skipping save.\n",
      "File train_data\\92b2d28b-21e4-498c-b6dd-c27a47716a25.parquet already exists. Skipping save.\n",
      "File train_data\\9a2e5b24-1d93-47ef-bd90-0fae0d719df7.parquet already exists. Skipping save.\n",
      "File validate_data\\a366ff0e-ac1e-4632-821f-594ee8750b90.parquet already exists. Skipping save.\n",
      "File validate_data\\b5a540db-434b-4c3d-86dd-4668d40586c2.parquet already exists. Skipping save.\n",
      "File validate_data\\ef4852a4-fcfe-429b-b753-d11e2ad08cac.parquet already exists. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "data_dir = './cache/raw'\n",
    "\n",
    "configurations = [\n",
    "    ('test_data', test_ids),\n",
    "    ('train_data', train_ids),\n",
    "    ('validate_data', validate_ids)\n",
    "]\n",
    "\n",
    "# after running get_common_columns (took nearly 2h) we can store the common columns to speed up the process\n",
    "# common columns: all columns which are at least in one flight not fully constant\n",
    "common_columns = ['6417134', '28827303', '45268905', '108790465', '117182271', '156751662', '156875280', '192589766', '203733126', '301190512', '340940040', '353872548', '362955152', '381802901', '419755350', '439066970', '491152707', '513685691', '518160218', '557098463', '564468419', '580739016', '610037603', '614270119', '630411227', '637219977', '638103778', '677695791', '683661709', '683679735', '705547489', '731865654', '738096043', '750466567', '754202368', '760531114', '786775081', '853651284', '855504072', '919467553', '943710121', '965636185', '1049384201', '1049691088', '1074440103', '1089322549', '1102716554', '1105598667', '1112206407', '1114842726', '1128128022', '1129907732', '1145477697', '1169465987', '1185350318', '1269872157', '1289350632', '1296641768', '1308053202', '1314928026', '1324901455', '1359970420', '1432743115', '1450782446', '1464341275', '1464691036', '1500331376', '1501357761', '1515452476', '1550781114', '1553784294', '1556708778', '1637431807', '1666704728', '1748585914', '1782666709', '1791424934', '1796138214', '1814846759', '1831637452', '1847367549', '1863207721', '1879505128', '1895873384', '1909389254', '1971234014', '1984892607', '2074986628', '2122819504', '2142867155', '2155694236', '2176349777', '2209591699', '2219783417', '2275361864', '2279678027', '2298125514', '2318747298', '2338387453', '2356154278', '2370500633', '2381936850', '2400822283', '2457184866', '2478351513', '2496586604', '2498379844', '2503582969', '2524343919', '2531864991', '2545498036', '2546226082', '2589702191', '2590494337', '2663217327', '2690567179', '2700477795', '2705846059', '2716830861', '2730101906', '2769157819', '2815095005', '2829673346', '2840355407', '2876656444', '2885619987', '2936255604', '2942017749', '2979223644', '2988014268', '3063191758', '3066527403', '3163423083', '3207956524', '3296928681', '3298183894', '3298276793', '3321734281', '3327788219', '3335916359', '3355805873', '3358452658', '3363211662', '3410338846', '3489008753', '3493828033', '3495907791', '3502630583', '3576014097', '3590140009', '3599019297', '3599416138', '3645293884', '3659998070', '3663402178', '3687766012', '3706315026', '3720232134', '3742756340', '3752743700', '3771464240', '3827725929', '3871827618', '3881043328', '3911364189', '3917330120', '3928845979', '3933014103', '3966668421', '3981973171', '4015812076', '4021288050', '4022486067', '4054750327', '4056189073', '4058842283', '4061964072', '4065458540', '4066202059', '4081583215', '4096114806', '4096188490', '4174711423', '4210968644', '4211937095', '4233706951', '4247030081', '4264003232', '4294814812']\n",
    "#common_columns = get_common_columns(data_dir, configurations)\n",
    "# after running the model and using lasso, we identified the necessarry columns: (when model/data changes, this could change)\n",
    "#common_columns = []\n",
    "\n",
    "for folder, ids in configurations:\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # Function to process and save Parquet file data\n",
    "    def save_parquet_data(flight_id):\n",
    "        print(f\"{datetime.now()}: Processing flight {flight_id}\")\n",
    "        parquet_path = os.path.join(data_dir, f\"{flight_id}.joblib\")\n",
    "\n",
    "        df = load(parquet_path)\n",
    "        df.columns = df.columns.map(str)\n",
    "        \n",
    "        # keep only the common columns in each DataFrame\n",
    "        df = df.reindex(columns=common_columns, fill_value=0)\n",
    "        \n",
    "        df = process_parquet_file(df)\n",
    "\n",
    "        df = columns_to_scalar(df) # [] -> float | string -> NaN | bool -> int | ...\n",
    "    \n",
    "        df = df.reindex(columns=common_columns, fill_value=0)\n",
    "\n",
    "        # add labels according to json\n",
    "        df = add_row_label(df, folder, flight_id)\n",
    "\n",
    "        # Define the base output file name\n",
    "        output_path = os.path.join(folder, f\"{flight_id}.parquet\")\n",
    "\n",
    "        # Save the DataFrame as a CSV file\n",
    "        df.columns = df.columns.map(str)\n",
    "        df.to_parquet(output_path, index=True)\n",
    "        print(f\"{datetime.now()}: Saved {flight_id} Parquet data to {output_path}\")\n",
    "\n",
    "    # Process each Parquet file and save it\n",
    "    for flight_id in ids:\n",
    "        # check if the file exists\n",
    "        output_path = os.path.join(folder, f\"{flight_id}.parquet\")\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"File {output_path} already exists. Skipping save.\")\n",
    "        else:\n",
    "            save_parquet_data(flight_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Cache parquet processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cache/validate_data\\a366ff0e-ac1e-4632-821f-594ee8750b90_parquet.joblib cached already\n",
      "Cached data to ./cache/validate_data\\b5a540db-434b-4c3d-86dd-4668d40586c2_parquet.joblib\n",
      "Cached data to ./cache/validate_data\\ef4852a4-fcfe-429b-b753-d11e2ad08cac_parquet.joblib\n"
     ]
    }
   ],
   "source": [
    "#output_dir = 'test_data'\n",
    "#current_ids = test_ids\n",
    "\n",
    "#output_dir = 'train_data'\n",
    "#current_ids = train_ids\n",
    "\n",
    "output_dir = 'validate_data'\n",
    "current_ids = validate_ids\n",
    "\n",
    "cache_dir = './cache/' + output_dir\n",
    "os.makedirs(cache_dir, exist_ok=True) \n",
    "\n",
    "# cache parquet_csv file\n",
    "for flight_id in current_ids:\n",
    "    file_path = os.path.join('./' + output_dir, f\"{flight_id}.parquet\")\n",
    "\n",
    "    cache_file = os.path.join(cache_dir, os.path.basename(file_path).replace('.parquet', '_parquet.joblib'))\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"{cache_file} cached already\")\n",
    "    else:\n",
    "        data = pd.read_parquet(file_path)\n",
    "        dump(data, cache_file)\n",
    "        print(f\"Cached data to {cache_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualize the data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
